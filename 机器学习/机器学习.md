# 第1章 机器学习概述

机器学习已经广泛应用于数据挖掘、搜索引擎、电子商务、自动驾驶、图像识别、量化投资、自然语言处理、计算机视觉、医学诊断、信用卡欺诈检测、证券金融市场分析、游戏和机器人等领域，机器学习相关技术的进步促进了人工智能在各个领域的发展。

## 1.1 机器学习简介

机器学习这门学科所关注的是计算机程序如何随着经验积累，自动提高性能。

机器学习主要的理论基础涉及概率论、数理统计、线性代数、数学分析、数值逼近、最优化理论和计算复杂理论等，其核心要素是数据、算法和模型。

### 1.1.1 机器学习简史

可以追溯到20世纪50年代

![机器学习演变过程](.\images\image-20210209170453766.png)

机器学习的发展分为知识推理期、知识工程期、浅层学习（ShallowLearning）和深度学习（Deep Learning）几个阶段

### 1.1.2 机器学习主要流派

1. 符号主义

   实现方法是用符号表示知识，并用规则进行逻辑推理，其中专家系统和知识工程是这一学说的代表性成果

2. 贝叶斯派

   P(A|B)是在事件B发生的情况下事件A发生的可能性（条件概率）。自然语言中的情感分类、自动驾驶和垃圾邮件过滤等

3. 联结主义

   主要算法是神经网络，由大量神经元以一定的结构组成

   在神经网络中，将n个相连接的神经元的输出作为当前神经元的输入，进行加权计算，并加一个偏置值（Bias）之后通过激活函数来实现变换，激活函数的作用是将输出控制在一定的范围以内。以Sigmoid函数为例，输入从负无穷到正无穷，经过激活之后映射到（0，1）区间。

   人工神经网络是以层（Layer）形式组织起来的，每一层中包含多个神经元，层与层之间通过一定的结构连接起来，对神经网络的训练目的就是要找到网络中各个突触连接的权重和偏置值。

4. 进化主义

5. 行为类推主义

   根据约束条件来优化函数，在实际应用中，就是计算它们之间的相似度，然后定义关联关系。

## 1.2 机器学习、人工智能和数据挖掘

![机器学习与人工智能额关系](\images\image-20210209171041747.png)

### 1.2.1 什么是人工智能

人工智能是让机器的行为看起来像人所表现出的智能行为一样。

人工智能的典型系统包括以下几个方面：

- （1）博弈游戏（如深蓝、Alpha Go、Alpha Zero等）。
- （2）机器人相关控制理论（运动规划、控制机器人行走等）。
- （3）机器翻译。
- （4）语音识别。
- （5）计算机视觉系统。
- （6）自然语言处理（自动程序）。

### 1.2.2 什么是数据挖掘

数据挖掘使用机器学习、统计学和数据库等方法在相对大量的数据集中发现模式和知识，它涉及数据预处理、模型与推断、可视化等。

数据挖掘包括以下几类常见任务：

1. 异常检测 

   异常检测（anomaly detection）是对不符合预期模式的样本、事件进行识别。

2. 关联分析

   关联规则学习（Association rule learning）是在数据库中发现变量之间的关系（强规则）。

3. 聚类

   发现数据的类别与结构。

4. 分类

   通过特征选择和学习，建立判别函数以对样本进行分类。

5. 回归

   目标是找出误差最小的拟合函数作为模型，用特定的自变量来预测因变量的值。

### 1.2.3 机器学习、人工智能与数据挖掘的关系

机器学习是人工智能的一个分支

数据挖掘和机器学习的交集越来越大，机器学习成为数据挖掘的重要支撑技术。

## 1.3 典型机器学习应用领域

1. 艺术创作
2. 金融领域
3. 医疗领域
4. 自然语言处理
   - 分词
   - 词性标注
   - 句法分析
   - 自然语言生成
   - 文本分类
   - 信息检索
   - 信息抽取
   - 文本校对
   - 问答系统
   - 机器翻译
   - 自动摘要

5. 网络安全
6. 工业领域
7. 机器学习在娱乐行业的应用

## 1.4 机器学习算法

机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的方法，可以分成下面几种类别：==监督学习、无监督学习、强化学习==。

- （1）监督学习是从有标记的训练数据中学习一个模型，然后根据这个模型对未知样本进行预测。其中，模型的输入是某一样本的特征，函数的输出是这一样本对应的标签。常见的监督学习算法包括回归分析和统计分类。监督学习包括分类和数字预测两大类别，前者包括逻辑回归、决策树、KNN、随机森林、支持向量机、朴素贝叶斯等，后者包括线性回归、KNN、GradientBoosting和AdaBoost等。
- （2）无监督学习又称为非监督式学习，它的输入样本并不需要标记，而是自动从样本中学习特征实现预测。常见的无监督学习算法有聚类和关联分析等，在人工神经网络中，自组织映射（SOM）和适应性共振理论（ART）是最常用的无监督学习。
- （3）强化学习是通过观察来学习做成什么样的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。强化学习强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。

### 1. 分类算法

分类算法是应用分类规则对记录进行目标映射，将其划分到不同的分类中，构建具有泛化能力的算法模型，即构建映射规则来预测未知样本的类别。

主要的分类算法包括==决策树、支持向量机（Support Vector Machine，SVM）、最近邻（K-Nearest Neighbor，KNN）算法、贝叶斯网络（BayesNetwork）和神经网络==等。

- 决策树

  决策树是一棵用于决策的树，目标类别作为叶子节点，特征属性的验证作为非叶子节点，而每个分支是特征属性的输出结果。决策树擅长对人物、位置、事物的不同特征、品质、特性进行评估，可应用于基于规则的信用评估、比赛结果预测等。决策过程是从根节点出发，测试不同的特征属性，按照结果的不同选择分支，最终落到某一叶子节点，获得分类结果。主要的决策树算法有ID3、C4.5、C5.0、CART、CHAID、SLIQ、SPRINT等。

- 支持向量机

  支持向量机（Support Vector Machine，SVM）是由瓦普尼克（Vapnik）等人设计的一种分类器，其主要思想是将低维特征空间中的线性不可分进行非线性映射，转化为高维空间的线性可分。

- 最近邻算法

  对样本应用向量空间模型表示，将相似度高的样本分为一类，对新样本计算与之距离最近（最相似）的样本的类别，那么新样本就属于这些样本中类别最多的那一类

  最近邻算法的主要缺点有：①在各分类样本数量不平衡时误差较大；②由于每次比较要遍历整个训练样本集来计算相似度，所以分类的效率较低，时间和空间复杂度较高；③近邻的数量选择不合理可能会导致结果的误差较大；④在原始近邻算法中没有权重的概念，所有特征采用相同的权重参数，这样计算出来的相似度易产生误差。

- 贝叶斯网络

  贝叶斯网络又称为置信网络（Belief Network），是基于贝叶斯定理绘制的具有概率分布的有向弧段图形化网络，其理论基础是贝叶斯公式，网络中的每个点表示变量，有向弧段表示两者间的概率关系。

- 神经网络

  神经网络包括==输入层、隐藏层、输出层==，每一个节点代表一个神经元，节点之间的连线对应权重值，输入变量经过神经元时会运行激活函数，对输入值赋予权重并加上偏置，将输出结果传递到下一层中的神经元，而权重值和偏置在神经网络训练过程中不断修正。

  神经网络的训练过程主要包括==前向传输==和==逆向反馈==

### 2. 聚类算法

聚类是基于无监督学习的分析模型，不需要对原始数据进行标记，按照数据的内在结构特征进行聚集形成簇群，从而实现数据的分离。聚类与分类的主要区别是其并不关心数据是什么类别，而是把相似的数据聚集起来形成某一类簇。

聚类方法可分为基于层次的聚类（Hierarchical Method）、基于划分的聚类（Partitioning Method，PAM）、基于密度的聚类、基于约束的聚类、基于网络的聚类等。

- BIRCH算法

  利用层次方法来平衡迭代规则和聚类，它只需要扫描数据集一次便可实现聚类，它利用了类似B+树的结构对样本集进行划分，叶子节点之间用双向链表进行连接，逐渐对树的结构进行优化获得聚类。

  BIRCH算法的主要优点是空间复杂度低，内存占用少，效率较高，能够对噪声点进行滤除。缺点是其树中节点的聚类特征树有个数限制，可能会产生与实际类别个数不一致的情况；而且对样本有一定的限制，要求数据集的样本是超球体，否则聚类的效果不佳。

- CURE算法

  传统的基于划分聚类的方法得到的是凸形的聚类，对异常数据较敏感，而CURE算法是使用多个代表点来替换聚类中的单个点，算法更加稳健。另外，在处理大数据时采用分区和随机取样，使其处理大数据量的样本集时效率更高，且不会降低聚类质量。

- k-均值算法

  传统的k-均值算法的聚类过程是在样本集中随机选择k个聚类中心点，对每个样本计算候选中心的距离进行分组，在得到分组之后重新计算类簇的中心，循环迭代直到聚类中心不变或收敛。k-均值存在较多改进算法，如初始化优化k-均值算法、距离优化Elkan k-Means算法、k-Prototype算法等。

- DBSCAN算法

  DBSCAN算法是基于样本之间的密度实现空间聚类，基于核心点、边界点和噪声点等因素对空间中任意形状的样本进行聚类。与传统的k-均值相比，DBSCAN通过邻域半径和密度阈值自动生成聚类，不需要指定聚类个数，支持过滤噪声点。但是当数据量增大时，算法的空间复杂度较高，DBSCAN不适用于样本间的密度不均匀的情况，否则聚类的质量较差。对于高维的数据，一方面密度定义比较难，另一方面会导致计算量较大，聚类效率较低。

- OPTICS算法

  在DBSCAN算法中，用户需要指定ε（邻域半径）和minPts（ε邻域最小点数）两个初始参数，用户手动设置这两个参数会对聚类结果产生比较关键的影响。而OPTICS解决了上述问题，为聚类分析生成一个增广的簇排序，代表了各样本点基于密度的聚类结构。

### 3. 关联分析

关联分析（Associative Analysis）是通过对数据集中某些项目同时出现的概率来发现它们之间的关联关系，其典型的应用是购物篮分析，通过分析购物篮中不同商品之间的关联，分析消费者的购买行为习惯，从而制定相应的营销策略，为商品促销、产品定价、位置摆放等提供支持，并且可用于对不同消费者群体的划分。关联分析主要包括Apriori算法和FP-growth算法。

- Apriori算法

  首先生成所有频繁项集，然后由频繁项集构造出满足最小置信度的规则。由于Apriori算法要多次扫描样本集，需要由候选频繁项集生成频繁项集，在处理大数据量数据时效率较低。

- FP-growth算法

  该算法只进行两次数据集扫描且不使用候选项集，直接按照支持度来构造一个频繁模式树，用这棵树生成关联规则，在处理比较大的数据集时效率比Apriori算法大约快一个数量级，对于海量数据，可以通过数据划分、样本采样等方法进行再次改进和优化。

- Eclat算法

  Eclat算法是一种深度优先算法，采用垂直数据表示形式，基于前缀的等价关系将搜索空间划分为较小的子空间，可以快速挖掘频繁项集。与FP-growth和Apriori算法不同，Eclat算法的核心思想是倒排，将事务数据中的事务主键与项目（item）进行转换，用项目作为主键，这样就可以直观看到每个项目对应的事务ID有哪些，方便计算项目的频次，从而快速获得频繁项集。

### 4. 回归分析

回归分析是一种研究自变量和因变量之间关系的预测模型，用于分析当自变量发生变化时因变量的变化值，要求自变量相互独立。

- 线性回归

  自变量是连续型，线性回归用直线（回归线）建立因变量和一个或多个自变量之间的关系。

  线性回归主要的特点如下。

  ① 自变量与因变量之间呈现线性关系。

  ② 多重共线性、自相关和异方差对多元线性回归的影响很大。

  ③ 线性回归对异常值非常敏感，其能影响预测值。

  ④ 在处理多个自变量时，需要用逐步回归的方法来自动选择显著性变量，不需要人工干预，其思想是将自变量逐个引入模型中，并进行F检验、t检验等来筛选变量，当新引入的变量对模型结果没有改进时，将其剔除，直到模型结果稳定。

- 逻辑回归

  逻辑（Logistic）回归是数据分析中的常用算法，其输出的是概率估算值，将此值用Sigmoid函数进行映射到[0，1]区间，即可用来实现样本分类。逻辑回归对样本量有一定要求，在样本量较少时，概率估计的误差较大。

- 多项式回归

  在回归分析中有时会遇到线性回归的直线拟合效果不佳，如果发现散点图中数据点呈多项式曲线时，可以考虑使用多项式回归来分析。使用多项式回归可以降低模型的误差，但是如果处理不当易造成模型过拟合，在回归分析完成之后需要对结果进行分析，并将结果可视化以查看其拟合程度。

- 岭回归

  岭回归在共线性数据分析中应用较多，也称为脊回归，它是一种有偏估计的回归方法，是在最小二乘估计法的基础上做了改进，通过舍弃最小二乘法的无偏性，使回归系数更加稳定和稳健。其中R方值会稍低于普通回归分析方法，但回归系数更加显著，主要用于变量间存在共线性和数据点较少时。

- LASSO回归

  LASSO回归的特点与岭回归类似，在拟合模型的同时进行变量筛选和复杂度调整。

### 5. 深度学习

深度学习方法是通过使用多个隐藏层和大量数据来学习特征，从而提升分类或预测的准确性，与传统的神经网络相比，不仅在层数上较多，而且采用了逐层训练的机制来训练整个网络，以防出现梯度扩散。

受限玻尔兹曼机（RBM）主要解决概率分布问题，是一种玻尔兹曼机的变体，基于物理学中的能量函数实现建模，“受限”是指层间存在连接，但层内的单元间不存在连接。

深度信念网络（DBN）是杰弗里·希尔顿（Geoffrey Hinton）在2006年提出的，作为早期深度生成式模型的代表，目标是建立一个样本数据和标签之间的联合分布。DBN由多个RBM层组成，RBM的层神经元分为可见神经元和隐性神经元，其中，接受输入的是可见神经元，隐神经元用于提取特征。通过训练神经元之间的权重，不仅可以用来识别特征、分类数据，还可以让整个神经网络按照最大概率来生成训练数据。

长短期记忆（Long Short-term Memory，LSTM）神经网络是循环神经网络的一种，尽管这个早期循环神经网络只允许留存少量的信息，但其形式会存在损耗，而LSTM有长期与短期的记忆，拥有更好的控制记忆的能力，避免梯度衰减或逐层传递的值的最终退化。

在卷积神经网络（Convolutional Neural Network）中，卷积是指将源数据与滤波矩阵进行内积操作，从而实现特征权重的融合，通过设置不同的滤波矩阵实现提取不同特征

## 1.5 机器学习的一般流程

机器学习的一般流程包括确定分析目标、收集数据、整理数据、预处理数据、训练模型、评估模型、优化模型、上线部署等步骤

# 第2章 机器学习基本方法