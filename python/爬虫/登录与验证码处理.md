## 处理登录表单

随着Web 2.0的发展，大量数据都由==用户==产生，这里需要用到页面==交互==，如在论坛提交一个帖子或发送一条微博。因此，处理表单和登录成为进行网络爬虫不可或缺的一部分。获取网页和提交表单相比，获取网页是从网页==抓取==数据，而提交表单是向网页==上传==数据。

在客户端（浏览器）向服务器提交HTTP请求的时候，两种常用到的方法是GET和POST。使用GET方法的时候，查询字符串（名称/值对）是在GET请求的URL中发送的，因为浏览器对URL有==长度限制==，所以GET请求提交的数据会有所限制。这里数据都清清楚楚地出现在URL中，所以GET请求不应在处理敏感数据时使用，如密码。

按照规定，GET请求只应用于获取数据，POST请求则用于提交数据。因为查询字符串（名称/值对）在POST请求的HTTP消息主体中，所以敏感数据不会出现在URL中，参数也不会被保存在浏览器历史或Web服务器日志中。表单数据的提交基本上要用到POST请求。

### 处理登录表单

处理登录表单可以分为两步：

- （1）研究网站登录表单，构建POST请求的参数字典。
- （2）提交POST请求。

```python
import  requests
session = requests.session() #创建一个session对象 session对象会存储特定用户会话所需的属性和配置信息，这对我们后面在其中保存和操作cookies非常有意义。
post_url = 'http://xxx.xx.com'
agent = ''
headers = {
    'Host':'xxx.xx.com',
    'Origin':'http://xxx.xx.com',
    'Referer':'http://www.xxx.xx.com/test-login',
    'User-Agent':agent
}
#这个要根据自己的需求来变更
postdata = {
    'pwd' :'123456',
    'user' :'test',
    'remember':'forever'
}

login_page = session.post(post_url,data=postdata,headers=headers)
print(login_page.status_code)
```

状态码：

- 303——重定向
- 400——请求错误
- 401——未授权
- 403——禁止访问
- 404——文件未找到
- 500——服务器错误

### 处理cookies，让网页记住你的登录

使用cookie能够把登录状态记录下来，再次运行代码的时候可以直接获取之前的登录状态，从而不用重新登录。

```python
import  requests
import http.cookiejar as cookielib #导入cookiejar库
session = requests.session()
session.cookies = cookielib.LWPCookieJar(filename='cookies') #说明cookies所在的位置
post_url = 'http://xxx.xx.com'
agent = ''
headers = {
    'Host':'xxx.xx.com',
    'Origin':'http://xxx.xx.com',
    'Referer':'http://www.xxx.xx.com/test-login',
    'User-Agent':agent
}
#这个要根据自己的需求来变更
postdata = {
    'pwd' :'123456',
    'user' :'test',
    'remember':'forever'
}

login_page = session.post(post_url,data=postdata,headers=headers)
print(login_page.status_code)
session.cookies.save() #保存此次登录的cookies。
```

再使用的时候就加载保存的cookie即可

```python
session = requests.session()
session.cookie = cookielib.LWPCookieJar(filename='cookies')
try:
    session.cookies.load(ignore_discard=True)
except:
    print('cookie未能加载')
```

如果没有出现“cookie未能加载”，就表示cookies已经加载成功了.

## 验证码的处理

验证码（CAPTCHA）是“Completely Automated Public Turing test totell Computers and Humans Apart”（全自动区分计算机和人类的图灵测试）的缩写，是一种区分用户是计算机还是人的公共全自动程序，可以防止恶意破解密码、刷票、论坛灌水，以及黑客用特定程序暴力破解密码的方式进行不断的登录尝试。

在网络爬虫中，处理验证码主要有两种方式：

- （1）人工输入处理。
- （2）OCR识别处理。

处理验证码和普通输入框一样的，使用审查元素获取input框即可。

### 人工方法处理验证码

人工方法处理就是在爬虫程序运行的时候弹出一个验证码输入框，我们需要手动输入验证码。

手工输入方式比较传统，不进行代码展示。

### OCR处理验证码

OCR（Optical Character Recognition，光学字符识别），也就是使用字符识别方法将形状翻译成计算机文字的过程。为了使用Python将图像识别为字母和数字，我们需要用到==Tesseract==库【pip install pytesseract】，它是Google支持的开源ocr项目。

```python
from PIL import Image
import pytesseract
# 把彩色图像转化为灰度图像。通过灰度处理可以把色彩空间由RGB转化为HIS
im = Image.open('test.jpg')
gray = im.convert('L')
gray.show()
gray.save('test_gray.jpg')

# 图片降噪
# 二值化处理。可以看到，验证码中文本的部分颜色都比较深，因此可以把大于某个临界灰度值的像素灰度设为灰度极大值，把小于这个值的像素灰度设为灰度极小值，从而实现二值化
threshold = 150
table = []
for i in range(256):
    if i <threshold:
        table.append(0)
    else:
        table.append(1)
out = gray.point(table,'1')
out.show()
out.save('test_thresholded.jpg')
#使用Tesseract进行图片识别
th = Image.open('test_thresholded.jpg')
print(pytesseract.image_to_string(th))

```

