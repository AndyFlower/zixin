## 项目目标

爬取维基百科上的词条链接

## 项目描述

找到该网站上的一个网页，如主页，获取主页的内容，分析网页内容并找到主页上所有的本站链接，然后爬取刚刚获得的链接，再分析这些链接上的网页内容，找到上面所有的本站链接，并不断重复，直到没有新的链接为止

## 深度优先和广度优先

深度优先的遍历可以描述为“不撞南墙不回头”，具体一点就是首先访问第一个邻接节点，然后以这个被访问的邻接节点作为初始节点，访问它的第一个邻接节点。访问策略是优先往纵向挖掘深入，直到到达指定的深度或该节点不存在邻接节点，才会掉头访问第二条路。

广度优先的遍历可以描述为“一层一层地剥开我的心”。具体一点就是从某个顶点出发，首先访问这个顶点，然后找出这个节点的所有未被访问的邻接节点，访问完后再访问这些节点中第一个邻接节点的所有节点，重复此方法，直到所有节点都被访问完为止。访问策略采用先访问完一个深度的所有节点，再访问更深一层的所有节点，并采用FIFO（先进先出）的策略。

### 深度优先的递归爬虫

```python
import requests
from bs4 import BeautifulSoup
import re
import  time
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36'
}
'''
r = requests.get('https://en.wikipedia.beta.wmflabs.org/wiki/Main_Page',headers=headers)
html = r.text
bsObj =BeautifulSoup(html)
re = '<a href="/wiki/([^:#=<>]*?)".*?</a>'
for link in bsObj.findAll('a'):
    if 'href' in link.attrs:
        print(link.attrs['href']) #得到该页面的所有链接
'''

exist_url = []#存放已爬取的网页
g_writecount = 0
def scrappy(url,depth=1):
    global g_writecount
    try:
        url = "https://en.wikipedia.org/wiki/"+url
        r = requests.get(url,headers=headers)
        html = r.text
    except Exception as e:
        print('Failed downloading and saving',url)
        print(e)
        exist_url.append(url)
        return None
    exist_url.append(url)
    link_list = re.findall('<a href= "/wiki/([^#=<>]*?)".*?</a>',html)
    # 去掉已爬链接和重复链接
    unique_list = list(set(link_list)-set(exist_url))
    # 把链接写入txt文件中
    for eachone in unique_list:
        g_writecount+=1
        output = "No."+str(g_writecount) +'\t Depth:'+str(depth) +'\t'+url+'->'+eachone + '\n'
        print(output)
        with open('link_12-3.txt','a+') as f:
            f.write(output)
        if depth<2:
            scrappy(eachone,depth+1)
scrappy('Wikipedia')
```

