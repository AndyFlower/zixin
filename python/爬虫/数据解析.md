## **引入**

- 回顾requests模块实现数据爬取的流程
  - 指定url
  - 发起请求
  - 获取响应数据
  - 持久化存储
- 其实，在上述流程中还需要较为重要的一步，就是在持久化存储之前需要进行指定数据解析。因为大多数情况下的需求，我们都会指定去使用聚焦爬虫，也就是爬取页面中指定部分的数据值，而不是整个页面的数据。至此，我们的数据爬取的流程可以修改为：
  - 指定url
  - 发起请求
  - 获取响应数据
  - **数据解析**
  - 持久化存储

## **python如何实现数据解析**

- **正则表达式**
- **xpath解析**
- **bs4解析**

## 数据解析原理概述

- 解析的局部的文本内容都会在标签直接或者标签对应的属性中进行存储
- 进行指定标签的定位
- 标签或者标签对应的属性中存储的数据值进行提取（解析）

## 常用正则表达式回顾

![image-20210201141030759](.\images\image-20210201141030759.png)

## **项目需求：爬取糗事百科指定页面的糗图，并将其保存到指定文件夹中**

```python
 # -*-  encoding=utf-8 -*-
import requests
import re
import  os

if __name__ =='__main__':
    url = 'https://www.qiushibaike.com/pic/%s/'
    # 进行UA伪装
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36'
    }
    # 指定起始页和结束页
    page_start = int(input('enter start page:'))
    page_end = int(input('enter end page:'))

    # 创建文件夹
    if not os.path.exists('images'):
        os.mkdir('images')
    for page in range(page_start,page_end):
        print('正在下载第%d页图片'%page)
        new_url = format(url % page)
        response = requests.get(url = new_url,headers=headers)
        #print(response.text)
        # 解析response中的图片链接
        e = '.*?.*?'
        ex = '<img src="(.*?)" alt.*? />'
        pa = re.compile(e,re.S)
        image_urls = re.findall(ex,response.text,re.S)
        #print(image_urls)
        for image_url in image_urls:
            print(image_url)
            image_url = 'https:'+image_url
            image_name=image_url.split('/')[-1].split('?')[0]
            #print(image_name)
            image_path = 'images/'+image_name
            try:
                image_data = requests.get(url=image_url,headers=headers).content
                with open(image_path, 'wb') as fp:
                    fp.write(image_data)
            except Exception as e:
                print(e)



```

## 数据解析---bs4解析

### **环境安装**

```
 需要将pip源设置为国内源，阿里源、豆瓣源、网易源等
   - windows
    （1）打开文件资源管理器(文件夹地址栏中)
    （2）地址栏上面输入 %appdata%
    （3）在这里面新建一个文件夹  pip
    （4）在pip文件夹里面新建一个文件叫做  pip.ini ,内容写如下即可
        [global]
        timeout = 6000
        index-url = https://mirrors.aliyun.com/pypi/simple/
        trusted-host = mirrors.aliyun.com
   - linux
    （1）cd ~
    （2）mkdir ~/.pip
    （3）vi ~/.pip/pip.conf
    （4）编辑内容，和windows一模一样
- 需要安装：pip install bs4
     bs4在使用时候需要一个第三方库，把这个库也安装一下
     pip install lxml
```

### **基础使用**

使用流程：       

- 导包：from bs4 import BeautifulSoup

- 使用方式：可以将一个html文档，转化为BeautifulSoup对象，然后通过对象的方法或者属性去查找指定的节点内容

  （1）转化本地文件：
               -  soup = BeautifulSoup(open('本地文件'), 'lxml')
  （2）转化网络文件：
               - soup = BeautifulSoup('字符串类型或者字节类型', 'lxml')
  （3）打印soup对象显示内容为html文件中的内容
-  基础巩固：
      （1）根据标签名查找
          soup.a   只能找到第一个符合要求的标签 a表示的是a标签
      （2）获取属性
          soup.a.attrs  获取a所有的属性和属性值，返回一个字典
          soup.a.attrs['href']   获取href属性
          soup.a['href']   也可简写为这种形式
      （3）获取内容
          soup.a.string
          soup.a.text
          soup.a.get_text()
         【注意】如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容
      （4）find：找到第一个符合要求的标签
           soup.find('a')  找到第一个符合要求的
          soup.find('a', title="xxx")
           soup.find('a', alt="xxx")
          soup.find('a', class_="xxx")
           soup.find('a', id="xxx")
      （5）find_all：找到所有符合要求的标签
           soup.find_all('a')
           soup.find_all(['a','b']) 找到所有的a和b标签
          soup.find_all('a', limit=2)  限制前两个
      （6）根据选择器选择指定的内容
                 select:soup.select('#feng')
          常见的选择器：标签选择器(a)、类选择器(.)、id选择器(#)、层级选择器
               层级选择器：
                  div .dudu #lala .meme .xixi  下面好多级
                  div > p > a > .lala          只能是下面一级
          【注意】select选择器返回永远是列表，需要通过下标提取指定的对象


