## **引入**

- 回顾requests模块实现数据爬取的流程
  - 指定url
  - 发起请求
  - 获取响应数据
  - 持久化存储
- 其实，在上述流程中还需要较为重要的一步，就是在持久化存储之前需要进行指定数据解析。因为大多数情况下的需求，我们都会指定去使用聚焦爬虫，也就是爬取页面中指定部分的数据值，而不是整个页面的数据。至此，我们的数据爬取的流程可以修改为：
  - 指定url
  - 发起请求
  - 获取响应数据
  - **数据解析**
  - 持久化存储

## **python如何实现数据解析**

- **正则表达式**
- **xpath解析**
- **bs4解析**

## 数据解析原理概述

- 解析的局部的文本内容都会在标签直接或者标签对应的属性中进行存储
- 进行指定标签的定位
- 标签或者标签对应的属性中存储的数据值进行提取（解析）

## 常用正则表达式回顾

![image-20210201141030759](.\images\image-20210201141030759.png)

###  re.match方法

从字符串==起始位置==匹配一个模式，如果从起始位置匹配不了，match()就返回none。

re.match的语法为re.match(pattern, string, flags=0)，其中pattern是正则表达式，包含一些特殊的字符，string为要匹配的字符串，flags用来控制正则表达式的匹配方式，如是否区分大小写、多行匹配等。

```python
import re
m = re.match('www', 'www.santostang.com')
print ("匹配的结果:  ", m)    
print ("匹配的起始与终点:  ", m.span()) 
print ("匹配的起始位置:  ", m.start())
print ("匹配的终点位置:  ", m.end())

```

> 匹配的结果:   <_sre.SRE_Match object; span=(0, 3), match='www'>
>
> 匹配的起始与终点:   (0, 3)
>
> 匹配的起始位置:   0
>
> 匹配的终点位置:   3

### re.search方法

re.match只能从字符串的起始位置进行匹配，而re.search扫描整个字符串并返回第一个成功的匹配

```python
import re
m_match = re.match('com', 'www.santostang.com')
m_search = re.search('com', 'www.santostang.com')
print (m_match)
print (m_search)
```

>None
>
><_sre.SRE_Match object; span=(15, 18), match='com'>

###  re.findall方法

上述match和search方法中，我们只能找到一个匹配所写的模式，而findall可以找到所有的匹配

```python
import re
m_match = re.match('[0-9]+', '12345 is the first number, 23456 is the sencond')
m_search = re.search('[0-9]+', 'The first number is 12345, 23456 is the sencond')
m_findall = re.findall('[0-9]+', '12345 is the first number, 23456 is the sencond')
print (m_match.group())
print (m_search.group())
print (m_findall)
```

> 12345
>
> 12345
>
> ['12345', '23456']

findall与match、search不同的是，findall能够找到所有匹配的结果，并且以列表的形式返回。

## **项目需求：爬取糗事百科指定页面的糗图，并将其保存到指定文件夹中**

```python
 # -*-  encoding=utf-8 -*-
import requests
import re
import  os

if __name__ =='__main__':
    url = 'https://www.qiushibaike.com/pic/%s/'
    # 进行UA伪装
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36'
    }
    # 指定起始页和结束页
    page_start = int(input('enter start page:'))
    page_end = int(input('enter end page:'))

    # 创建文件夹
    if not os.path.exists('images'):
        os.mkdir('images')
    for page in range(page_start,page_end):
        print('正在下载第%d页图片'%page)
        new_url = format(url % page)
        response = requests.get(url = new_url,headers=headers)
        #print(response.text)
        # 解析response中的图片链接
        e = '.*?.*?'
        ex = '<img src="(.*?)" alt.*? />'
        pa = re.compile(e,re.S)
        image_urls = re.findall(ex,response.text,re.S)
        #print(image_urls)
        for image_url in image_urls:
            print(image_url)
            image_url = 'https:'+image_url
            image_name=image_url.split('/')[-1].split('?')[0]
            #print(image_name)
            image_path = 'images/'+image_name
            try:
                image_data = requests.get(url=image_url,headers=headers).content
                with open(image_path, 'wb') as fp:
                    fp.write(image_data)
            except Exception as e:
                print(e)



```

## 数据解析---bs4解析

### **环境安装**

```
 需要将pip源设置为国内源，阿里源、豆瓣源、网易源等
   - windows
    （1）打开文件资源管理器(文件夹地址栏中)
    （2）地址栏上面输入 %appdata%
    （3）在这里面新建一个文件夹  pip
    （4）在pip文件夹里面新建一个文件叫做  pip.ini ,内容写如下即可
        [global]
        timeout = 6000
        index-url = https://mirrors.aliyun.com/pypi/simple/
        trusted-host = mirrors.aliyun.com
   - linux
    （1）cd ~
    （2）mkdir ~/.pip
    （3）vi ~/.pip/pip.conf
    （4）编辑内容，和windows一模一样
- 需要安装：pip install bs4
     bs4在使用时候需要一个第三方库，把这个库也安装一下
     pip install lxml
```

### 解析器

![image-20210202171412798](.\images\image-20210202171412798.png)

### **基础使用**

使用流程：       

- 导包：from bs4 import BeautifulSoup

- 使用方式：可以将一个html文档，转化为BeautifulSoup对象，然后通过对象的方法或者属性去查找指定的节点内容

  （1）转化本地文件：
               -  soup = BeautifulSoup(open('本地文件'), 'lxml')
  （2）转化网络文件：
               - soup = BeautifulSoup('字符串类型或者字节类型', 'lxml')
  （3）打印soup对象显示内容为html文件中的内容
-  基础巩固：
      （1）根据标签名查找
          soup.a   只能找到第一个符合要求的标签 a表示的是a标签
      （2）获取属性
          soup.a.attrs  获取a所有的属性和属性值，返回一个字典
          soup.a.attrs['href']   获取href属性
          soup.a['href']   也可简写为这种形式
      （3）获取内容
          soup.a.string
          soup.a.text
          soup.a.get_text()
         【注意】如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容
      （4）find：找到第一个符合要求的标签
           soup.find('a')  找到第一个符合要求的
          soup.find('a', title="xxx")
           soup.find('a', alt="xxx")
          soup.find('a', class_="xxx")
           soup.find('a', id="xxx")
      （5）find_all：找到所有符合要求的标签
           soup.find_all('a')
           soup.find_all(['a','b']) 找到所有的a和b标签
          soup.find_all('a', limit=2)  限制前两个
      （6）根据选择器选择指定的内容
                 select:soup.select('#feng')
          常见的选择器：标签选择器(a)、类选择器(.)、id选择器(#)、层级选择器
               层级选择器：
                  div .dudu #lala .meme .xixi  下面好多级
                  div > p > a > .lala          只能是下面一级
          【注意】select选择器返回永远是列表，需要通过下标提取指定的对象

   soup.prettify() :代码美化

### BeautifulSoup提取对象的方法

#### 遍历文档树

- 文档树的遍历方法就好像爬树一样，需要先爬到树干上，然后慢慢到小树干，最后到树枝上，就可以得到需要的数据了。

```python
soup.header.h1
```



- 对于某个标签的所有子节点，我们可以用contents把它的子节点以列表的方式输出：

```python
soup.header.div.contents
```

- 我们也可以使用children方法获得所有子标签：

```python
for child in soup.header.div.children:
    print(child)
```

- 获得所有子子孙孙的节点，就要用．descendants方法

```python
for child in soup.header.div.descendants:
    print(child)
```

- 获取父节点内容

```python
a_tag = soup.header.div.a
a_tag.parent
```



#### 搜索文档树

- find()
- find_all()

#### css选择器

- 可以通过tag标签逐层查找

  ```python
  soup.select("header h1")
  ```

- 通过某个tag标签下的直接子标签遍历

  ```python
  soup.select("header > h3")
  soup.select("div > a")
  ```

## 使用lxml解析网页

> 安装 pip install lxml

使用lxml提取网页源代码数据也有3种方法，即XPath选择器、CSS选择器和BeautifulSoup的find()方法

XPath是一门在XML文档中查找信息的语言。XPath使用路径表达式来选取XML文档中的节点或节点集，也可以用在HTML获取数据中。

```python
import requests
from lxml import etree

link = "http://www.santostang.com/"
headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} 
r = requests.get(link, headers= headers)

html = etree.HTML(r.text)
title_list = html.xpath('//h1[@class="post-title"]/a/text()')
print (title_list)
```

"//h1"代表选取所有<h1>子元素，"//"无论在文档中什么位置，后面加上[@class="post-title"]表示选取<h1>中class为"post-title"的元素，/a表示选取<h1>子元素的<a>元素，/text()表示提取<a>元素中的所有文本。

xpath可以通过chrome浏览器检查中复制出来